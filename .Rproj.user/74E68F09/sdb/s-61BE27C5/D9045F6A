{
    "contents" : "library(XML)\nlibrary(RCurl)\nlibrary(tm)\nlibrary(tmcn)\nlibrary(Rwordseg)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nlibrary(vegetarian)\nsource(\"myDict.R\")\n#-----------------------------------------\n# 處理字詞庫\n#-----------------------------------------\n\n# 匯入sogou字庫\nwords1 <- toTrad(readLines(\"http://wubi.sogou.com/dict/download_txt.php?id=9182\")) # ptt字庫\nwords2 <- toTrad(readLines(\"http://wubi.sogou.com/dict/download_txt.php?id=9912\")) # 繁體字庫\nwords <- c(words1,words2)\ninsertWords(words)\n\n# 自建字庫\nstrwords <-  c(\"服貿\", \"服贸\", \"馬英九\", \"江宜樺\", \"立法院\", \"國會\", \"行政院\", \"魏揚\", \"林飛帆\", \"陳為廷\", \"台灣\", \n               \"警察\", \"暴力\", \"鎮暴警察\", \"學運\", \"黑色島國\", \"清大\", \"台大\", \"鎮壓\", \"後退\", \"張慶忠\", \"王金平\",\n               \"蘋果\", \"陪審團\", \"粉絲團\", \"蘋論\", \"陣線\", \"最新\", \"評論\", \"獨立\", \"媒體\", \"每日\", \"總覽\", \"有話\", \"要說\" ,\"即時\", \"論壇\")\ninsertWords(strwords, strtype=rep(\"n\", length(strwords)), numfreq=rep(1000, length(strwords)))\n\n# 定義停詞\nmyStopWords <- c(toTrad(stopwordsCN()), \"編輯\", \"時間\", \"標題\", \"發信\", \"實業\", \"作者\", \"要聞\", \"即時新聞\", \"聯合新聞網\", \"全文網址\", \"全文\", \"網址\", \n                 \"大家\", \"今天\", \"知道\", \"非常\", \"很多\", \"現在\", \"希望\", \"不要\", \"已經\", \"看到\", \"謝謝\", \"其實\", \"事情\",\n                 \"蘋果\", \"陪審團\", \"粉絲團\", \"蘋論\", \"陣線\", \"最新\", \"評論\", \"獨立\", \"媒體\", \"每日\", \"總覽\", \"有話\", \"要說\" ,\"即時\", \"論壇\",\n                 \"投稿\", \"報導\", \"新聞\", \"表示\", \"粉絲\", \"沒有\", \"青島\", \"院內\", \"濟南\", \"現場\", \"主持人\", \"場內\", \"一起\", \"出來\", \"一下\", \"裡面\", \"可能\", \"需要\",\n                 \"應該\", \"覺得\", \"繼續\", \"告訴\", \"不能\", \"剛剛\", \"接下來\", \"下去\", \"廣播\", \"訊息\", \"可能\", )\n\n#----------------------------------------------------------\n# 爬ptt 服貿版文章 http://www.ptt.cc/bbs/FuMouDiscuss/\n#----------------------------------------------------------\ndata <- list()\nfor( i in 1:565){ \n  tmp <- paste(i, '.html', sep='')\n  if(i<100) tmp <- paste('0',i,'.html',sep='')\n  url <- paste('www.ptt.cc/bbs/FuMouDiscuss/index', tmp, sep='')\n  html <- htmlParse(getURL(url))\n  url.list <- xpathSApply(html, \"//div[@class='title']/a[@href]\", xmlAttrs)\n  data <- rbind(data, paste('www.ptt.cc', url.list, sep=''))\n}\ndata <- unlist(data)\n\n# 利用所有文章的url連結去抓所有文章的html網頁, 並用xpathSApply去解析出文章的內容並儲存\ngetdoc <- function(line){\n  start <- regexpr('www', line)[1]\n  end <- regexpr('html', line)[1]\n  if(start != -1 & end != -1){\n    url <- substr(line, start, end+3)\n    html <- htmlParse(getURL(url), encoding='UTF-8')\n    doc <- xpathSApply(html, \"//div[@id='main-content']\", xmlValue)\n    name <- strsplit(url, '/')[[1]][4]\n    write(doc, gsub('html', 'txt', name))\n  }      \n}\n\nif(length(dir(\"ptt\"))==0){\n  setwd(\"ptt\")\n  sapply(data, getdoc)  # 爬服貿版文章內容\n  setwd(\"../\")\n}\nfeq <- list()\nfeq[[1]] <- myText(input=\"live/\", output=paste(Sys.Date(),\"-live字頻.txt\", sep=\"\"), method=\"dir\") \nfeq[[2]] <- myText(input=\"ptt/\", output=paste(Sys.Date(),\"-PTT字頻.txt\", sep=\"\"), method=\"dir\") \n\n\n\n#-----------------------------------------\n# 整理各家新聞資料\n#-----------------------------------------\nraw <- read.csv(\"0325/0325-output.csv\")\nnews <- raw[!duplicated(raw$標題),] # 只擷取第一次刊登的新聞\nnews$新聞來源 <- factor(news$新聞來源, levels=c(1:10,12:14), \n                    labels=c(\"蘋果\", \"中時\", \"中央社\", \"東森\", \"自由\", \"新頭殼\", \"NowNews\", \n                             \"聯合\", \"TVBS\", \"中廣\", \"台視\", \"華視\", \"民視\"))\ncontent <- news[,c(\"新聞來源\", \"內容\")]\n\n# 計算各家媒體的關鍵字頻\nfor(i in 1:13){\n  id <- which(content$新聞來源==levels(content$新聞來源)[i])\n  if(length(id)!=0){\n    input <- as.character(content[id,\"內容\"])\n    feq[[i+2]] <- myText(input=input, output=paste(Sys.Date(),\"-\",levels(content$新聞來源)[i],\"字頻.txt\", sep=\"\"))\n  }\n}\n\n\n# 計算關鍵字頻的相似度\ntmp <- out <- list()\nu <- as.character(unique(unlist(lapply(feq, function(x)as.character(x$word)))))\ntmp <- lapply(feq, function(x) rep(as.character(x$word), x$freq))\nout <- lapply(tmp, function(x) table(factor(x, levels=u, labels=u)))\ntab <- do.call(\"rbind\", out)\n# tab <- sweep(tab,MARGIN=1,STATS=rowSums(tab),FUN=\"/\")\nrownames(tab) <- c(\"Live\", \"PTT\", levels(content$新聞來源))\nsim <- sim.table(tab,q=2,half = FALSE)\nsim <- sim[lower.tri(sim)]\nk <- 0\nedges <- data.frame()\nfor(i in 1:(nrow(tab)-1)){\n  for(j in (i+1):nrow(tab)){\n    k <- k + 1\n    tmp <- data.frame(\"Source\"=i, \"Target\"=j, Type=\"Undirected\", \"Weight\"=sim[k])\n    edges <- rbind(edges, tmp)\n  }\n}\n\nnode <- data.frame(\"id\"=1:nrow(tab), \"label\"=c(\"Live\", \"PTT\", levels(content$新聞來源)), \"x\"=rep(1/nrow(tab),nrow(tab)))\nwrite.csv(node, paste(Sys.Date(),\"-\",\"node.csv\",sep=\"\"))\nwrite.csv(edges, paste(Sys.Date(),\"-\",\"edges.csv\",sep=\"\"))\na <- edges$Source \nb <- edges$Target\nid1 <- which(a==1 | a==2 | a==3 | a==4 | a==7 | a==10)\nid2 <- which(b==1 | b==2 | b==4 | b==4 | b==7 | b==10)\nwrite.csv(edges[intersect(id1,id2),], paste(Sys.Date(),\"-\",\"edges(紙本新聞).csv\",sep=\"\"))\n\nwrite.csv(t(tab), paste(Sys.Date(),\"-\",\"新聞字頻.csv\",sep=\"\"))\n\n\n",
    "created" : 1395810186083.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "942016072",
    "id" : "D9045F6A",
    "lastKnownWriteTime" : 1395931214,
    "path" : "~/GitHub/ecfa/myText2.R",
    "project_path" : "myText2.R",
    "properties" : {
        "tempName" : "Untitled2"
    },
    "source_on_save" : false,
    "type" : "r_source"
}