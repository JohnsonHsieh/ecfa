{
    "contents" : "library(XML)\nlibrary(RCurl)\nlibrary(tm)\nlibrary(tmcn)\nlibrary(Rwordseg)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\n\nlibrary(vegan)\n\nwords1 <- readLines(\"http://wubi.sogou.com/dict/download_txt.php?id=9182\") # ptt字庫\nwords2 <- readLines(\"http://wubi.sogou.com/dict/download_txt.php?id=9912\") # 繁體字庫\nwords <- toTrad(c(words1,words2))\n\nstrwords <-  c(\"服貿\", \"服贸\", \"馬英九\", \"江宜樺\", \"立法院\", \"國會\", \"行政院\", \"魏揚\", \"林飛帆\", \"陳為廷\", \n               \"警察\", \"暴力\", \"鎮暴警察\", \"學運\", \"黑色島國\", \"清大\", \"台大\", \"鎮壓\", \"後退\", \"張慶忠\", \"王金平\")\ninsertWords(strwords, strtype=rep(\"n\", length(strwords)), numfreq=rep(1000, length(strwords)))\n\ninsertWords(words)\nmyStopWords <- c(toTrad(stopwordsCN()), \"編輯\", \"時間\", \"標題\", \"發信\", \"實業\", \"作者\", \"要聞\", \"即時新聞\", \"聯合新聞網\", \"全文網址\", \"全文\", \"網址\", \n                 \"大家\", \"今天\", \"知道\", \"非常\", \"很多\", \"現在\", \"希望\", \"不要\", \"已經\", \"看到\", \"謝謝\", \"其實\", \"事情\")\n\n\n#----------------------------------------------------------\n# 爬ptt 服貿版文章 http://www.ptt.cc/bbs/FuMouDiscuss/\n#----------------------------------------------------------\ndata <- list()\nfor( i in 38:289){ #38~289是3/24號00:00至18:00的所有文章\n  tmp <- paste(i, '.html', sep='')\n  if(i<100) tmp <- paste('0',i,'.html',sep='')\n  url <- paste('www.ptt.cc/bbs/FuMouDiscuss/index', tmp, sep='')\n  html <- htmlParse(getURL(url))\n  url.list <- xpathSApply(html, \"//div[@class='title']/a[@href]\", xmlAttrs)\n  data <- rbind(data, paste('www.ptt.cc', url.list, sep=''))\n}\ndata <- unlist(data)\n\n# 利用所有文章的url連結去抓所有文章的html網頁, 並用xpathSApply去解析出文章的內容並儲存\ngetdoc <- function(line){\n  start <- regexpr('www', line)[1]\n  end <- regexpr('html', line)[1]\n  \n  if(start != -1 & end != -1){\n    url <- substr(line, start, end+3)\n    html <- htmlParse(getURL(url), encoding='UTF-8')\n    doc <- xpathSApply(html, \"//div[@id='main-content']\", xmlValue)\n    name <- strsplit(url, '/')[[1]][4]\n    write(doc, gsub('html', 'txt', name))\n  }      \n}\n\nsetwd(\"ptt\")\nsapply(data, getdoc)  # 爬服貿版文章內容\nsetwd(\"../\")\n\n#--------------------------------------------------------\n# 爬蘋果日報文章 \n# http://www.appledaily.com.tw/realtimenews/article/politics/20140324/366426/\n# http://www.appledaily.com.tw/realtimenews/article/politics/20140324/366301/\n#--------------------------------------------------------\nsetwd(\"apple/\")\nfor( i in 366329:366426){ #38~289是3/24號00:00至18:00的所有文章\n  tmp <- paste(i, '/', sep='')\n  url <- paste('http://www.appledaily.com.tw/realtimenews/article/politics/20140324/', tmp, sep='')\n  html <- htmlParse(getURL(url),encoding='UTF-8')\n  doc <- xpathSApply(html, \"//p[@id='summary']\", xmlValue)\n  write(doc, paste(i,'.txt',sep=\"\"))\n}\nsetwd(\"../\")\n\n\n#--------------------------------------------------------\n# 聯合新聞 未完成\n# http://fe3.udn.com/search/udnsearch.jsp?project=&Keywords=%AA%41%B6%54+&f_PAGE=1\n# http://fe3.udn.com/search/udnsearch.jsp?project=&Keywords=%AA%41%B6%54+&f_PAGE=50\n#--------------------------------------------------------\nsetwd(\"udn/\")\ndata <- list()\nfor( i in 1:50){ \n  tmp <- paste(i)\n  url <- paste('http://fe3.udn.com/search/udnsearch.jsp?project=&Keywords=%AA%41%B6%54+&f_PAGE=', tmp, sep='')\n  html <- htmlParse(getURL(url),encoding='UTF-8')\n  url.list <- xpathSApply(html, \"//dt/a[@href]\", xmlAttrs)\n  data[[i]] <- url.list[1,]\n}\ndata <- unlist(data)\n\n\n\n\n\n\n# 把文章進行分詞, 匯出名詞的頻率 \nmyText <- function(fileDir=\"g0v\", output=\"g0v-freq.txt\"){\n  d.corpus <- Corpus(DirSource(fileDir), list(language = NA))\n  \n  d.corpus <- tm_map(d.corpus, removePunctuation) #清除標點符號\n  d.corpus <- tm_map(d.corpus, removeNumbers) #清除數字\n  d.corpus <- tm_map(d.corpus, function(word) { #清除英文字母\n    gsub(\"[A-Za-z0-9]\", \"\", word)\n  })\n  \n  d.corpus <- tm_map(d.corpus, segmentCN, nature = TRUE)\n  d.corpus <- tm_map(d.corpus, function(sentence) {\n    noun <- lapply(sentence, function(w) {\n      w[(names(w) == \"n\")] # 只比較名詞\n    })\n    unlist(noun)\n  })\n  \n  d.corpus <- Corpus(VectorSource(d.corpus))\n  d.corpus <- tm_map(d.corpus, removeWords, myStopWords)\n  tdm <- TermDocumentMatrix(d.corpus, control = list(wordLengths = c(2, Inf)))\n  m1 <- as.matrix(tdm)\n  v <- sort(rowSums(m1), decreasing = TRUE)\n  d <- data.frame(word = names(v), freq = v)\n  write.table(data.frame(freq=d$freq, word=d$word), file=output,quote=FALSE,sep=\"\\t\",row.names=FALSE, col.names=FALSE)\n  \n  data.frame(freq=d$freq, word=d$word)\n}\nfau <- list()\nfau[[1]] <- myText(fileDir=\"0325/g0v\", output=\"g0v-freq.txt\") \nfau[[2]] <- myText(fileDir=\"0324/ptt\", output=\"ptt-freq.txt\") # 讀取很久 請慎用\nfau[[2]] <- fau[[2]][-(3:6),]\nfau[[3]] <- myText(fileDir=\"0324/apple\", output=\"apple-freq.txt\") # 蘋果日報\nfau[[4]] <- myText(fileDir=\"0324/ct\", output=\"ct-freq.txt\") # 中時電子報\nfau[[5]] <- myText(fileDir=\"0324/udn\", output=\"udn-freq.txt\") # 聯合新聞網\n\n\n\nmyPng <- function(input=fau0, output=\"wordcloud.png\",min.freq=2){\n  library(RColorBrewer)\n  pal2 <- brewer.pal(8,\"Dark2\")\n  par(family=\"Arial Unicode MS\")\n  png(output, width=800,height=800)\n  wordcloud(input$word,input$freq, scale=c(10,.4),min.freq=min.freq,\n            max.words=Inf, random.order=FALSE, colors=pal2)\n  dev.off()\n}\n\n# myPng(fau[[1]], \"g0v-wordcloud.png\")\n# myPng(fau[[2]], \"ptt-wordcloud.png\",min.freq=100)\n# myPng(fau[[3]], \"apple-wordcloud.png\")\n# myPng(fau[[4]], \"ct-wordcloud.png\")\n# myPng(fau[[5]], \"udn-wordcloud.png\")\n\nu <- as.character(unique(unlist(lapply(fau, function(x)x$word))))\n\ntmp <- out <- list()\ntmp <- lapply(fau, function(x) rep(x$word, x$freq))\nout <- lapply(tmp, function(x) table(factor(x, levels=u, labels=u)))\ntab <- do.call(\"rbind\", out)\nsim <- c(1 - vegdist(tab, method=\"morisita\")) #兩兩相似矩陣\nk <- 0\nedges <- data.frame()\nfor(i in 1:(nrow(tab)-1)){\n  for(j in (i+1):nrow(tab)){\n    k <- k +1\n    tmp <- data.frame(\"Source\"=i, \"Target\"=j, Type=\"Undirected\", \"Weight\"=sim[k])\n    edges <- rbind(edges, tmp)\n  }\n}\nedges\n\n\n\nnode <- data.frame(\"id\"=1:nrow(tab), \"x\"=rep(1/nrow(tab),nrow(tab)))\n\nwrite.csv(node, \"node.csv\")\nwrite.csv(edges, \"edges.csv\")\n",
    "created" : 1395761761258.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1183328780",
    "id" : "68172F00",
    "lastKnownWriteTime" : 1395781418,
    "path" : "~/GitHub/ecfa/myText.R",
    "project_path" : "myText.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}